{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7764110b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.math import argmax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f235a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Networks\n",
    "def get_actor(num_states, num_actions, continuous, disc_actions_num, layer1, layer2,\n",
    "              init_weights_min=-0.003, init_weights_max=0.003):\n",
    "    \n",
    "    last_init = tf.random_uniform_initializer(minval=init_weights_min, maxval=init_weights_max)\n",
    "    \n",
    "    ### ACTOR NETWORK ###\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(inputs)\n",
    "    out = layers.LayerNormalization(axis=1)(out)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "    out = layers.LayerNormalization(axis=1)(out)\n",
    "    if continuous:\n",
    "        outputs = layers.Dense(num_actions, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "    else:\n",
    "        outputs = layers.Dense(disc_actions_num, activation=\"softmax\", kernel_initializer=last_init)(out)\n",
    "    \n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "def get_critic(num_states, num_actions, continuous, disc_actions_num, layer1, layer2):\n",
    "    \n",
    "    ### CRITIC NETWORK ###\n",
    "    \n",
    "    state_input = layers.Input(shape=(num_states,))\n",
    "    state_out = layers.Dense(64, activation=\"relu\")(state_input)\n",
    "    \n",
    "    if continuous:\n",
    "        action_input = layers.Input(shape=(num_actions,))\n",
    "    else:\n",
    "        action_input = layers.Input(shape=(disc_actions_num,))\n",
    "    action_out = layers.Dense(64, activation=\"relu\")(action_input)\n",
    "\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(concat)\n",
    "    out = layers.LayerNormalization(axis=1)(out)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "    out = layers.LayerNormalization(axis=1)(out)\n",
    "    outputs = layers.Dense(num_actions)(out)\n",
    "\n",
    "    return tf.keras.Model([state_input, action_input], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df8df9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta, dt, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "    def __call__(self):\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))\n",
    "        \n",
    "def fixed(x, episode):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d09a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions, continuous,\n",
    "            buffer_capacity, batch_size, std_dev, actor_lr, critic_lr,\n",
    "            gamma, tau, epsilon, adam_critic_eps, adam_actor_eps, \n",
    "            actor_amsgrad, critic_amsgrad, actor_layer_1, actor_layer_2, \n",
    "            critic_layer_1, critic_layer_2, theta, dt, disc_actions_num, loss_func):\n",
    "        \n",
    "        self.continuous = continuous\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        # This is used to make sure we only sample from used buffer space\n",
    "        self.buffer_counter = 0\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        if self.continuous:\n",
    "            self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        else:\n",
    "            self.action_buffer = np.zeros((self.buffer_capacity, disc_actions_num))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.done_buffer = np.zeros((self.buffer_capacity, 1)).astype(np.float32)\n",
    "        self.std_dev = std_dev\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_lr = actor_lr\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.epsilon = epsilon # Epsilon greedy\n",
    "        self.loss_func = loss_func\n",
    "        \n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1), theta=theta, dt=dt)\n",
    "        \n",
    "        self.actor_model = get_actor(num_states, num_actions, continuous, disc_actions_num, actor_layer_1, actor_layer_2)\n",
    "        self.critic_model = get_critic(num_states, num_actions, continuous, disc_actions_num, critic_layer_1, critic_layer_2)\n",
    "        self.target_actor = get_actor(num_states, num_actions, continuous, disc_actions_num, actor_layer_1, actor_layer_2)\n",
    "        self.target_critic = get_critic(num_states, num_actions, continuous, disc_actions_num, critic_layer_1, critic_layer_2)\n",
    "        \n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=actor_lr, beta_1=0.9, beta_2=0.999, epsilon=adam_actor_eps, amsgrad=actor_amsgrad,\n",
    "        )\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=critic_lr, beta_1=0.9, beta_2=0.999, epsilon=adam_critic_eps, amsgrad=critic_amsgrad,\n",
    "        )\n",
    "        # Making the weights equal initially\n",
    "        self.target_actor.set_weights(self.actor_model.get_weights())\n",
    "        self.target_critic.set_weights(self.critic_model.get_weights())\n",
    "    \n",
    "    def record(self, obs_tuple):\n",
    "        # Reuse the same buffer replacing old entries\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.done_buffer[index] = obs_tuple[4]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "    \n",
    "    # Calculation of loss and gradients\n",
    "    @tf.function\n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch, loss_func):\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + done_batch * self.gamma * self.target_critic([next_state_batch, target_actions], training=True)\n",
    "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = loss_func(y, critic_value)\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        critic_gvd = zip(critic_grad, self.critic_model.trainable_variables)\n",
    "        critic_capped_grad = [(tf.clip_by_value(grad, clip_value_min=-1, clip_value_max=1), var) for grad, var in critic_gvd]\n",
    "        \n",
    "        self.critic_optimizer.apply_gradients(critic_capped_grad)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor_model(state_batch, training=True)\n",
    "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "\n",
    "        actor_gvd = zip(actor_grad, self.actor_model.trainable_variables)\n",
    "        actor_capped_grad = [(tf.clip_by_value(grad, clip_value_min=-1, clip_value_max=1), var) for grad, var in actor_gvd]\n",
    "        \n",
    "        self.actor_optimizer.apply_gradients(actor_capped_grad)\n",
    "\n",
    "    def learn(self):\n",
    "        # Sample only valid data\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        done_batch = tf.convert_to_tensor(self.done_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch, done_batch, self.loss_func)\n",
    "        \n",
    "    def policy(self, state, disc_actions_num=4, noise_object=0, use_noise=True, noise_mult=1, rng=np.random.default_rng(1)):\n",
    "        if use_noise:\n",
    "            if self.continuous:\n",
    "                sampled_actions = tf.squeeze(self.actor_model(state))\n",
    "                \n",
    "                noise = noise_object()\n",
    "                \n",
    "                sampled_actions = sampled_actions.numpy() + noise * noise_mult\n",
    "\n",
    "                # We make sure action is within bounds\n",
    "                legal_action = np.clip(sampled_actions, -1, 1)\n",
    "                return [np.squeeze(legal_action)][0]\n",
    "            else:\n",
    "                if (rng.random() < self.epsilon):\n",
    "                    action = np.zeros(disc_actions_num)\n",
    "                    action[np.random.randint(0, disc_actions_num, 1)[0]] = 1\n",
    "                    return action\n",
    "                else:\n",
    "                    return self.actor_model(state)\n",
    "        else:    \n",
    "            if self.continuous:\n",
    "                sampled_actions = tf.squeeze(self.actor_model(state)).numpy()\n",
    "                legal_action = np.clip(sampled_actions, -1, 1)\n",
    "                return [np.squeeze(legal_action)][0]\n",
    "            else:\n",
    "                return self.actor_model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42f9f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(env, total_trials=1, total_episodes=100, buffer_capacity=50000, batch_size=64, \n",
    "            std_dev=0.3, critic_lr=0.003, render=False,\n",
    "            actor_lr=0.002, gamma=0.99, tau=0.005, noise_mult=1, save_weights=True, \n",
    "            directory='Weights/', gamma_func=fixed, tau_func=fixed, critic_lr_func=fixed, actor_lr_func=fixed,\n",
    "            noise_mult_func=fixed, std_dev_func=fixed, mean_number=20, output=True,\n",
    "            return_rewards=False, total_time=True, reward_mod=False, solved=200,\n",
    "            continuous=True, seed=1453, start_steps=0,\n",
    "            epsilon=0.2, epsilon_func=fixed, adam_critic_eps=1e-07, adam_actor_eps=1e-07,\n",
    "            actor_amsgrad=False, critic_amsgrad=False, actor_layer_1=256, actor_layer_2=256,\n",
    "            critic_layer_1=256, critic_layer_2=256, theta=0.15, dt=1e-2, disc_actions_num=4,\n",
    "            loss_func=losses.MeanAbsoluteError(), use_gpu=True):\n",
    "    \n",
    "    tot_time = time.time()\n",
    "    \n",
    "    _ = env.reset(seed=seed)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    try:\n",
    "        continuous = env.continuous\n",
    "    except:\n",
    "        continuous = True\n",
    "    \n",
    "    if not use_gpu:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "        \n",
    "    num_states = env.observation_space.low.shape[0]\n",
    "    if continuous:\n",
    "        num_actions = env.action_space.shape[0]\n",
    "    else:\n",
    "        num_actions = 1\n",
    "\n",
    "    # Normalize action space according to https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
    "    env.action_space = spaces.Box(low=-1, high=1, shape=(num_actions,), dtype='float32')\n",
    "    \n",
    "    ep_reward_list = []\n",
    "    avg_reward_list = []\n",
    "    true_reward_list = []\n",
    "    true_avg_reward_list = []\n",
    "    \n",
    "    for trial in range(total_trials):\n",
    "        step = 0\n",
    "\n",
    "        # Add sublists for each trial\n",
    "        avg_reward_list.append([])\n",
    "        ep_reward_list.append([])\n",
    "        true_reward_list.append([])\n",
    "        true_avg_reward_list.append([])\n",
    "        \n",
    "        agent = Agent(num_states, num_actions, continuous,\n",
    "            buffer_capacity, batch_size, std_dev, actor_lr, critic_lr,\n",
    "            gamma, tau, epsilon, adam_critic_eps, adam_actor_eps, \n",
    "            actor_amsgrad, critic_amsgrad, actor_layer_1, actor_layer_2, \n",
    "            critic_layer_1, critic_layer_2, theta, dt, disc_actions_num, loss_func)\n",
    "\n",
    "        for ep in range(total_episodes):\n",
    "            before = time.time()\n",
    "            \n",
    "            agent.gamma = gamma_func(agent.gamma, ep)\n",
    "            agent.tau = tau_func(agent.tau, ep)\n",
    "            agent.critic_lr = critic_lr_func(agent.critic_lr, ep)\n",
    "            agent.actor_lr = actor_lr_func(agent.actor_lr, ep)\n",
    "            agent.std_dev = std_dev_func(agent.std_dev, ep)\n",
    "            agent.epsilon = epsilon_func(agent.epsilon, ep)\n",
    "            noise_mult = noise_mult_func(noise_mult, ep)\n",
    "\n",
    "            prev_state = env.reset()\n",
    "            episodic_reward = 0\n",
    "            true_reward = 0\n",
    "\n",
    "            while True:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                \n",
    "                tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "                if step >= start_steps:\n",
    "                    action = agent.policy(state=tf_prev_state, disc_actions_num=disc_actions_num, noise_object=agent.ou_noise, noise_mult=noise_mult, rng=rng)\n",
    "                else:\n",
    "                    action = env.action_space.sample()\n",
    "                \n",
    "                step += 1\n",
    "                \n",
    "                if continuous:\n",
    "                    try:\n",
    "                        len(action)\n",
    "                    except:\n",
    "                        action = [action]\n",
    "                    state, reward, done, info = env.step(action)\n",
    "                else:\n",
    "                    state, reward, done, info = env.step(np.argmax(action))\n",
    "                \n",
    "                true_reward += reward\n",
    "                \n",
    "                # Reward modification\n",
    "                if reward_mod:\n",
    "                    reward -= abs(state[0])\n",
    "\n",
    "                terminal_state = int(not done)\n",
    "                \n",
    "                agent.record((prev_state, action, reward, state, terminal_state))\n",
    "\n",
    "                agent.learn()\n",
    "                update_target(agent.target_actor.variables, agent.actor_model.variables, agent.tau)\n",
    "                update_target(agent.target_critic.variables, agent.critic_model.variables, agent.tau)\n",
    "\n",
    "                episodic_reward += reward\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                prev_state = state\n",
    "\n",
    "            ep_reward_list[trial].append(episodic_reward)\n",
    "            avg_reward = np.mean(ep_reward_list[trial][-mean_number:])\n",
    "            avg_reward_list[trial].append(avg_reward)\n",
    "            true_reward_list[trial].append(true_reward)\n",
    "            true_avg_reward = np.mean(true_reward_list[trial][-mean_number:])\n",
    "            true_avg_reward_list[trial].append(true_avg_reward)\n",
    "            \n",
    "            if output:\n",
    "                print(\"Ep {} * AvgReward {:.2f} * true AvgReward {:.2f} * Reward {:.2f} * True Reward {:.2f} * time {:.2f} * step {}\"\n",
    "                  .format(ep, avg_reward, true_avg_reward, episodic_reward, true_reward, (time.time() - before), step))\n",
    "            \n",
    "            # Stop if avg is above 'solved'\n",
    "            if true_avg_reward >= solved:\n",
    "                break\n",
    "\n",
    "        # Save weights\n",
    "        now = datetime.datetime.now()\n",
    "        timestamp = \"{}.{}.{}.{}.{}.{}\".format(now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "        save_name = \"{}_{}_{}\".format(env.spec.id, continuous, timestamp)\n",
    "        if save_weights:\n",
    "            try:\n",
    "                agent.actor_model.save_weights(directory + 'actor-trial' + str(trial) + '_' + save_name + '.h5')\n",
    "            except:\n",
    "                print('actor save fail')\n",
    "            try:\n",
    "                agent.critic_model.save_weights(directory + 'critic-trial' + str(trial) + '_' + save_name + '.h5')\n",
    "            except:\n",
    "                print('critic save fail')\n",
    "    \n",
    "    # Plotting graph\n",
    "    for idx, p in enumerate(true_avg_reward_list):\n",
    "        plt.plot(p, label=str(idx))\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True Avg. Epsiodic Reward (\" + str(mean_number) + \")\")\n",
    "    plt.legend()\n",
    "    try:\n",
    "        plt.savefig('Graphs/' + save_name + '.png')\n",
    "    except:\n",
    "        print('fig save fail')\n",
    "    plt.show()\n",
    "    \n",
    "    print('total time:', time.time() - tot_time, 's')\n",
    "    \n",
    "    if return_rewards:\n",
    "        return true_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a57bcf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env, actor_weights, total_episodes=10, render=False, disc_actions_num=4, seed=1453):\n",
    "    rewards = []\n",
    "    \n",
    "    _ = env.reset(seed=seed)\n",
    "    \n",
    "    try:\n",
    "        continuous = env.continuous\n",
    "    except:\n",
    "        continuous = True\n",
    "        \n",
    "    num_states = env.observation_space.low.shape[0]\n",
    "    if continuous:\n",
    "        num_actions = env.action_space.shape[0]\n",
    "    else:\n",
    "        num_actions = 1\n",
    "\n",
    "    # Normalize action space according to https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
    "    env.action_space = spaces.Box(low=-1, high=1, shape=(num_actions,), dtype='float32')\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        agent = Agent(num_states=num_states, num_actions=num_actions, continuous=continuous, \n",
    "                buffer_capacity=0, batch_size=0)\n",
    "        agent.actor_model.load_weights(actor_weights)\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "            action = agent.policy(state=tf_prev_state, disc_actions_num=disc_actions_num, use_noise=False)\n",
    "\n",
    "            if continuous:\n",
    "                try:\n",
    "                    len(action)\n",
    "                except:\n",
    "                    action = [action]\n",
    "                state, reward, done, info = env.step(action)\n",
    "            else:\n",
    "                state, reward, done, info = env.step(np.argmax(action))\n",
    "            \n",
    "            ep_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(str(time.time() - before) + 's')\n",
    "                rewards.append(ep_reward)\n",
    "                break\n",
    "\n",
    "            prev_state = state\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a90fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random(env, total_episodes=10, render=False, seed=1453):\n",
    "    \n",
    "    _ = env.reset(seed=seed)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    try:\n",
    "        continuous = env.continuous\n",
    "    except:\n",
    "        continuous = True\n",
    "        \n",
    "    num_states = env.observation_space.low.shape[0]\n",
    "    if continuous:\n",
    "        num_actions = env.action_space.shape[0]\n",
    "    else:\n",
    "        num_actions = 1\n",
    "\n",
    "    # Normalize action space according to https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
    "    env.action_space = spaces.Box(low=-1, high=1, shape=(num_actions,), dtype='float32')\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(str(time.time() - before) + 's')\n",
    "                rewards.append(ep_reward)\n",
    "                break\n",
    "\n",
    "            prev_state = state\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83b8ba",
   "metadata": {},
   "source": [
    "---\n",
    "# Runs and tests\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "d017e55760df337c783d27f50334e290979defe406527979977afc7530ad3789"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
