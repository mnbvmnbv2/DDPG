{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7764110b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.math import argmax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f235a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Networks\n",
    "def get_actor(num_states, num_actions, continuous, disc_actions_num, layer1, layer2,\n",
    "              init_weights_min=-0.003, init_weights_max=0.003):\n",
    "    \n",
    "    last_init = tf.random_uniform_initializer(minval=init_weights_min, maxval=init_weights_max)\n",
    "    \n",
    "    ### ACTOR NETWORK ###\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(inputs)\n",
    "    out = layers.LayerNormalization(axis=1)(out)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "    out = layers.LayerNormalization(axis=1)(out)\n",
    "    if continuous:\n",
    "        outputs = layers.Dense(num_actions, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "    else:\n",
    "        outputs = layers.Dense(disc_actions_num, activation=\"softmax\", kernel_initializer=last_init)(out)\n",
    "    \n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "def get_critic(num_states, num_actions, continuous, disc_actions_num, layer1, layer2):\n",
    "    \n",
    "    ### CRITIC NETWORK ###\n",
    "    \n",
    "    state_input = layers.Input(shape=(num_states,))\n",
    "    state_out = layers.Dense(64, activation=\"relu\")(state_input)\n",
    "    \n",
    "    if continuous:\n",
    "        action_input = layers.Input(shape=(num_actions,))\n",
    "    else:\n",
    "        action_input = layers.Input(shape=(disc_actions_num,))\n",
    "    action_out = layers.Dense(64, activation=\"relu\")(action_input)\n",
    "\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(concat)\n",
    "    out = layers.LayerNormalization(axis=1)(out)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "    out = layers.LayerNormalization(axis=1)(out)\n",
    "    outputs = layers.Dense(num_actions)(out)\n",
    "\n",
    "    return tf.keras.Model([state_input, action_input], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df8df9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta, dt, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "    def __call__(self):\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))\n",
    "        \n",
    "def fixed(x, episode):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d09a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions, continuous,\n",
    "            buffer_capacity, batch_size, std_dev, actor_lr, critic_lr,\n",
    "            gamma, tau, epsilon, adam_critic_eps, adam_actor_eps, \n",
    "            actor_amsgrad, critic_amsgrad, actor_layer_1, actor_layer_2, \n",
    "            critic_layer_1, critic_layer_2, theta, dt, disc_actions_num, loss_func):\n",
    "        \n",
    "        self.continuous = continuous\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        # This is used to make sure we only sample from used buffer space\n",
    "        self.buffer_counter = 0\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        if self.continuous:\n",
    "            self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        else:\n",
    "            self.action_buffer = np.zeros((self.buffer_capacity, disc_actions_num))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.done_buffer = np.zeros((self.buffer_capacity, 1)).astype(np.float32)\n",
    "        self.std_dev = std_dev\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_lr = actor_lr\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.epsilon = epsilon # Epsilon greedy\n",
    "        self.loss_func = loss_func\n",
    "        \n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1), theta=theta, dt=dt)\n",
    "        \n",
    "        self.actor_model = get_actor(num_states, num_actions, continuous, disc_actions_num, actor_layer_1, actor_layer_2)\n",
    "        self.critic_model = get_critic(num_states, num_actions, continuous, disc_actions_num, critic_layer_1, critic_layer_2)\n",
    "        self.target_actor = get_actor(num_states, num_actions, continuous, disc_actions_num, actor_layer_1, actor_layer_2)\n",
    "        self.target_critic = get_critic(num_states, num_actions, continuous, disc_actions_num, critic_layer_1, critic_layer_2)\n",
    "        \n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=actor_lr, beta_1=0.9, beta_2=0.999, epsilon=adam_actor_eps, amsgrad=actor_amsgrad,\n",
    "        )\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=critic_lr, beta_1=0.9, beta_2=0.999, epsilon=adam_critic_eps, amsgrad=critic_amsgrad,\n",
    "        )\n",
    "        # Making the weights equal initially\n",
    "        self.target_actor.set_weights(self.actor_model.get_weights())\n",
    "        self.target_critic.set_weights(self.critic_model.get_weights())\n",
    "    \n",
    "    def record(self, obs_tuple):\n",
    "        # Reuse the same buffer replacing old entries\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.done_buffer[index] = obs_tuple[4]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "    \n",
    "    # Calculation of loss and gradients\n",
    "    @tf.function\n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch, loss_func):\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + done_batch * self.gamma * self.target_critic([next_state_batch, target_actions], training=True)\n",
    "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = loss_func(y, critic_value)\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        critic_gvd = zip(critic_grad, self.critic_model.trainable_variables)\n",
    "        critic_capped_grad = [(tf.clip_by_value(grad, clip_value_min=-1, clip_value_max=1), var) for grad, var in critic_gvd]\n",
    "        \n",
    "        self.critic_optimizer.apply_gradients(critic_capped_grad)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor_model(state_batch, training=True)\n",
    "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "\n",
    "        actor_gvd = zip(actor_grad, self.actor_model.trainable_variables)\n",
    "        actor_capped_grad = [(tf.clip_by_value(grad, clip_value_min=-1, clip_value_max=1), var) for grad, var in actor_gvd]\n",
    "        \n",
    "        self.actor_optimizer.apply_gradients(actor_capped_grad)\n",
    "\n",
    "    def learn(self):\n",
    "        # Sample only valid data\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        done_batch = tf.convert_to_tensor(self.done_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch, done_batch, self.loss_func)\n",
    "        \n",
    "    def policy(self, state, disc_actions_num=4, noise_object=0, use_noise=True, noise_mult=1, rng=np.random.default_rng(1)):\n",
    "        if use_noise:\n",
    "            if self.continuous:\n",
    "                sampled_actions = tf.squeeze(self.actor_model(state))\n",
    "                \n",
    "                noise = noise_object()\n",
    "                \n",
    "                sampled_actions = sampled_actions.numpy() + noise * noise_mult\n",
    "\n",
    "                # We make sure action is within bounds\n",
    "                legal_action = np.clip(sampled_actions, -1, 1)\n",
    "                return [np.squeeze(legal_action)][0]\n",
    "            else:\n",
    "                if (rng.random() < self.epsilon):\n",
    "                    action = np.zeros(disc_actions_num)\n",
    "                    action[np.random.randint(0, disc_actions_num, 1)[0]] = 1\n",
    "                    return action\n",
    "                else:\n",
    "                    return self.actor_model(state)\n",
    "        else:    \n",
    "            if self.continuous:\n",
    "                sampled_actions = tf.squeeze(self.actor_model(state)).numpy()\n",
    "                legal_action = np.clip(sampled_actions, -1, 1)\n",
    "                return [np.squeeze(legal_action)][0]\n",
    "            else:\n",
    "                return self.actor_model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42f9f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(env, total_trials=1, total_episodes=100, buffer_capacity=50000, batch_size=64, \n",
    "            std_dev=0.3, critic_lr=0.003, render=False,\n",
    "            actor_lr=0.002, gamma=0.99, tau=0.005, noise_mult=1, save_weights=True, \n",
    "            directory='Weights/', gamma_func=fixed, tau_func=fixed, critic_lr_func=fixed, actor_lr_func=fixed,\n",
    "            noise_mult_func=fixed, std_dev_func=fixed, mean_number=20, output=True,\n",
    "            return_rewards=False, total_time=True, reward_mod=False, solved=200,\n",
    "            continuous=True, seed=1453, start_steps=0,\n",
    "            epsilon=0.2, epsilon_func=fixed, adam_critic_eps=1e-07, adam_actor_eps=1e-07,\n",
    "            actor_amsgrad=False, critic_amsgrad=False, actor_layer_1=256, actor_layer_2=256,\n",
    "            critic_layer_1=256, critic_layer_2=256, theta=0.15, dt=1e-2, disc_actions_num=4,\n",
    "            loss_func=losses.MeanAbsoluteError(), use_gpu=True):\n",
    "    \n",
    "    tot_time = time.time()\n",
    "    \n",
    "    _ = env.reset(seed=seed)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    try:\n",
    "        continuous = env.continuous\n",
    "    except:\n",
    "        continuous = True\n",
    "    \n",
    "    if not use_gpu:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "        \n",
    "    num_states = env.observation_space.low.shape[0]\n",
    "    if continuous:\n",
    "        num_actions = env.action_space.shape[0]\n",
    "    else:\n",
    "        num_actions = 1\n",
    "\n",
    "    # Normalize action space according to https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
    "    env.action_space = spaces.Box(low=-1, high=1, shape=(num_actions,), dtype='float32')\n",
    "    \n",
    "    ep_reward_list = []\n",
    "    avg_reward_list = []\n",
    "    true_reward_list = []\n",
    "    true_avg_reward_list = []\n",
    "    \n",
    "    for trial in range(total_trials):\n",
    "        step = 0\n",
    "\n",
    "        # Add sublists for each trial\n",
    "        avg_reward_list.append([])\n",
    "        ep_reward_list.append([])\n",
    "        true_reward_list.append([])\n",
    "        true_avg_reward_list.append([])\n",
    "        \n",
    "        agent = Agent(num_states, num_actions, continuous,\n",
    "            buffer_capacity, batch_size, std_dev, actor_lr, critic_lr,\n",
    "            gamma, tau, epsilon, adam_critic_eps, adam_actor_eps, \n",
    "            actor_amsgrad, critic_amsgrad, actor_layer_1, actor_layer_2, \n",
    "            critic_layer_1, critic_layer_2, theta, dt, disc_actions_num, loss_func)\n",
    "\n",
    "        for ep in range(total_episodes):\n",
    "            before = time.time()\n",
    "            \n",
    "            agent.gamma = gamma_func(agent.gamma, ep)\n",
    "            agent.tau = tau_func(agent.tau, ep)\n",
    "            agent.critic_lr = critic_lr_func(agent.critic_lr, ep)\n",
    "            agent.actor_lr = actor_lr_func(agent.actor_lr, ep)\n",
    "            agent.std_dev = std_dev_func(agent.std_dev, ep)\n",
    "            agent.epsilon = epsilon_func(agent.epsilon, ep)\n",
    "            noise_mult = noise_mult_func(noise_mult, ep)\n",
    "\n",
    "            prev_state = env.reset()\n",
    "            episodic_reward = 0\n",
    "            true_reward = 0\n",
    "\n",
    "            while True:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                \n",
    "                tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "                if step >= start_steps:\n",
    "                    action = agent.policy(state=tf_prev_state, disc_actions_num=disc_actions_num, noise_object=agent.ou_noise, noise_mult=noise_mult, rng=rng)\n",
    "                else:\n",
    "                    action = env.action_space.sample()\n",
    "                \n",
    "                step += 1\n",
    "                \n",
    "                if continuous:\n",
    "                    try:\n",
    "                        len(action)\n",
    "                    except:\n",
    "                        action = [action]\n",
    "                    state, reward, done, info = env.step(action)\n",
    "                else:\n",
    "                    state, reward, done, info = env.step(np.argmax(action))\n",
    "                \n",
    "                true_reward += reward\n",
    "                \n",
    "                # Reward modification\n",
    "                if reward_mod:\n",
    "                    reward -= abs(state[0])\n",
    "\n",
    "                terminal_state = int(not done)\n",
    "                \n",
    "                agent.record((prev_state, action, reward, state, terminal_state))\n",
    "\n",
    "                agent.learn()\n",
    "                update_target(agent.target_actor.variables, agent.actor_model.variables, agent.tau)\n",
    "                update_target(agent.target_critic.variables, agent.critic_model.variables, agent.tau)\n",
    "\n",
    "                episodic_reward += reward\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                prev_state = state\n",
    "\n",
    "            ep_reward_list[trial].append(episodic_reward)\n",
    "            avg_reward = np.mean(ep_reward_list[trial][-mean_number:])\n",
    "            avg_reward_list[trial].append(avg_reward)\n",
    "            true_reward_list[trial].append(true_reward)\n",
    "            true_avg_reward = np.mean(true_reward_list[trial][-mean_number:])\n",
    "            true_avg_reward_list[trial].append(true_avg_reward)\n",
    "            \n",
    "            if output:\n",
    "                print(\"Ep {} * AvgReward {:.2f} * true AvgReward {:.2f} * Reward {:.2f} * True Reward {:.2f} * time {:.2f} * step {}\"\n",
    "                  .format(ep, avg_reward, true_avg_reward, episodic_reward, true_reward, (time.time() - before), step))\n",
    "            \n",
    "            # Stop if avg is above 'solved'\n",
    "            if true_avg_reward >= solved:\n",
    "                break\n",
    "\n",
    "        # Save weights\n",
    "        now = datetime.datetime.now()\n",
    "        timestamp = \"{}.{}.{}.{}.{}.{}\".format(now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "        save_name = \"{}_{}_{}\".format(env.spec.id, continuous, timestamp)\n",
    "        if save_weights:\n",
    "            try:\n",
    "                agent.actor_model.save_weights(directory + 'actor-trial' + str(trial) + '_' + save_name + '.h5')\n",
    "            except:\n",
    "                print('actor save fail')\n",
    "            try:\n",
    "                agent.critic_model.save_weights(directory + 'critic-trial' + str(trial) + '_' + save_name + '.h5')\n",
    "            except:\n",
    "                print('critic save fail')\n",
    "    \n",
    "    # Plotting graph\n",
    "    for idx, p in enumerate(true_avg_reward_list):\n",
    "        plt.plot(p, label=str(idx))\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True Avg. Epsiodic Reward (\" + str(mean_number) + \")\")\n",
    "    plt.legend()\n",
    "    try:\n",
    "        plt.savefig('Graphs/' + save_name + '.png')\n",
    "    except:\n",
    "        print('fig save fail')\n",
    "    plt.show()\n",
    "    \n",
    "    print('total time:', time.time() - tot_time, 's')\n",
    "    \n",
    "    if return_rewards:\n",
    "        return true_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a57bcf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env, actor_weights, total_episodes=10, render=False, disc_actions_num=4, seed=1453):\n",
    "    rewards = []\n",
    "    \n",
    "    _ = env.reset(seed=seed)\n",
    "    \n",
    "    try:\n",
    "        continuous = env.continuous\n",
    "    except:\n",
    "        continuous = True\n",
    "        \n",
    "    num_states = env.observation_space.low.shape[0]\n",
    "    if continuous:\n",
    "        num_actions = env.action_space.shape[0]\n",
    "    else:\n",
    "        num_actions = 1\n",
    "\n",
    "    # Normalize action space according to https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
    "    env.action_space = spaces.Box(low=-1, high=1, shape=(num_actions,), dtype='float32')\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        agent = Agent(num_states=num_states, num_actions=num_actions, continuous=continuous, \n",
    "                buffer_capacity=0, batch_size=0)\n",
    "        agent.actor_model.load_weights(actor_weights)\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "            action = agent.policy(state=tf_prev_state, disc_actions_num=disc_actions_num, use_noise=False)\n",
    "\n",
    "            if continuous:\n",
    "                try:\n",
    "                    len(action)\n",
    "                except:\n",
    "                    action = [action]\n",
    "                state, reward, done, info = env.step(action)\n",
    "            else:\n",
    "                state, reward, done, info = env.step(np.argmax(action))\n",
    "            \n",
    "            ep_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(str(time.time() - before) + 's')\n",
    "                rewards.append(ep_reward)\n",
    "                break\n",
    "\n",
    "            prev_state = state\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a90fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random(env, total_episodes=10, render=False, seed=1453):\n",
    "    \n",
    "    _ = env.reset(seed=seed)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    try:\n",
    "        continuous = env.continuous\n",
    "    except:\n",
    "        continuous = True\n",
    "        \n",
    "    num_states = env.observation_space.low.shape[0]\n",
    "    if continuous:\n",
    "        num_actions = env.action_space.shape[0]\n",
    "    else:\n",
    "        num_actions = 1\n",
    "\n",
    "    # Normalize action space according to https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
    "    env.action_space = spaces.Box(low=-1, high=1, shape=(num_actions,), dtype='float32')\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(str(time.time() - before) + 's')\n",
    "                rewards.append(ep_reward)\n",
    "                break\n",
    "\n",
    "            prev_state = state\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83b8ba",
   "metadata": {},
   "source": [
    "---\n",
    "# Runs and tests\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dfd838a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ferdinand\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "015a8ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnvSpec(id='Pendulum-v1', entry_point='gym.envs.classic_control.pendulum:PendulumEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, kwargs={}, namespace=None, name='Pendulum', version=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5072ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0 * AvgReward -1684.61 * true AvgReward -1684.61 * Reward -1684.61 * True Reward -1684.61 * time 9.53 * step 200\n",
      "Ep 1 * AvgReward -1657.45 * true AvgReward -1657.45 * Reward -1630.28 * True Reward -1630.28 * time 6.69 * step 400\n",
      "Ep 2 * AvgReward -1684.77 * true AvgReward -1684.77 * Reward -1739.42 * True Reward -1739.42 * time 6.69 * step 600\n",
      "Ep 3 * AvgReward -1666.54 * true AvgReward -1666.54 * Reward -1611.83 * True Reward -1611.83 * time 6.71 * step 800\n",
      "Ep 4 * AvgReward -1562.86 * true AvgReward -1562.86 * Reward -1148.15 * True Reward -1148.15 * time 6.70 * step 1000\n",
      "actor save fail\n",
      "critic save fail\n",
      "fig save fail\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 37.27134895324707 s\n"
     ]
    }
   ],
   "source": [
    "run(env=env, total_episodes=5, render=True, disc_actions_num=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636cefd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
